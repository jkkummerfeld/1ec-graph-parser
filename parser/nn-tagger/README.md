# Spine tagger

This folder contains a tagger for spines.
It is a modified form of the bidirection LSTM tagger included as an example in DyNet.

## Building

This requires several things to be installed already:

- C++ Boost (I used 1.63.0)
- DyNet
- Eigen (used by DyNet)
- Google Protocol Buffers

Note that while most of Boost is just header files, we use the serialization library, which must be compiled.

The makefile in this directory should generate the protocol buffer code and build the spine-tagger.

### Training

First you will need to convert the data to the correct format:

```Shell
python3 ./convert-for-tagger.py <  example-data.shp | python3 ./add_sent_id.py > eample-data.tag-format.txt
```

Then train the tagger:

```Shell
./spine-tagger
  --dynet-mem 6000
  -train train.tag-format.txt
  -dev dev.tag-format.txt
  -layers 2
  -input-dim 128
  -hidden-dim 256
  -tag-hidden-dim 64
  -prefix results.spine-tagger
```

## Running

If you start with tokenised sentences there are several steps in the process:

### Adding sentence IDs

To take a file of tokenised lines and add sentence IDs in our format, run:

```Shell
python3 add_sent_id.py < example-data.tok > example-data-with-ids.tok
```

This file is what the parser will be run with (so it knows the sentence IDs).
The reason we need IDs is that we can get the same sentence with different tag sequences (this occurs in the PTB annotations, so for oracle experiments using gold tags we need to be robust to it).
If you wish to start the IDs from a number other than 1, pass the starting number as an argument to the program.

### Simplifying the text

We convert to lowercase, make all numbers 0, and add some placeholders needed for formatting reasons:

```Shell
python3 pre-process.py < example-data-with-ids.tok > example-data-with-ids.tok.simple
```

### Run the tagger

You will need the model and dictionaries (available [here](https://www.dropbox.com/s/m0jjylo1mantz7q/Kummerfeld-Klein-2017.tagger.models.tgz?dl=0)).

```Shell
./spine-tagger -word-dict example.dict.words -tag-dict example.dict.tags -model model.params -test example-data-with-ids.tok.simple -prefix example
```

Note, this does not actually return the 1-best tag sequence by default.
Instead, it generates tag distributions that can then be used for pruning.

## Options

- `-train corpus.txt` Set the training data
- `-dev dev.txt` Set the development data
- `-test test.txt` Set the test data
- `-model model.params` Read the model from this file (when doing training it is automatically created)
- `-word-dict dict.words` Read the word dictionary from this file (when doing training it is automatically created)
- `-tag-dict dict.tags` Read the tag dictionary from this file (when doing training it is automatically created)
- `-prefix name` Prefix to use for all output files on this run (default = 'tagger-experiment')
- `-cutoff-ratio CUTOFF_RATIO` When running in test mode, spines with scores below this ratio of the best will not be kept, which is useful for returning small files (default = 0.5)
- `-layers LAYERS` Number of LSTM layers (default = 2)
- `-input-dim INPUT_DIM` Dimensionality of the vectors used for words (default = 128)
- `-hidden-dim HIDDEN_DIM` Dimensionality of the vectors in hidden states (default = 256)
- `-tag-hidden-dim TAG_HIDDEN_DIM` Dimensionality of the vectors used for tags (default = 64)
- `-print-labels` This will print the 1-best spines generated by the tagger

For training I suggest increasing the memory DyNet uses with `--dynet-mem 6000` and setting the number of threads for the Intel MKL library with `export MKL_NUM_THREADS=4`.

